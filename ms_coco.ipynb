{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Look at Images While Talking About Other Images:\n",
    "### A Quick Summary of Microsoft's _Common Objects in Context_ Image Database\n",
    "\n",
    "[It's been said](https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/#615638c36f63) that data scientists spend 60% of their time cleaning data, and 19% of their time building data sets, this leaves only 21% of a data scientist's time to allocate toward other tasks, such as:\n",
    " * building models\n",
    " * iteratively improving those models \n",
    " * going to meetings that should have been emails  \n",
    " \n",
    "Many of the most exciting parts about machine learning (and Data Science in general) are stuck behind a lot of fairly boring and mundane tasks.  \n",
    "\n",
    "![Let's talk about that](https://media1.tenor.com/images/2f93a5bd8c8ab67d92cd039ced056d14/tenor.gif?itemid=12726853)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2015, [a paper was released](https://arxiv.org/pdf/1405.0312.pdf) that describes \"over 70,000 worker hours\" spent building an enormous data set of labeled photographs. The goal of this data set was to provide training data for machine learning models to help with \"scene understanding\". At the time, there already existed multiple image data sets (which I'll summarize shortly), that focused on both high variety of objects, as well as many instances per object-type. One major downfall that Microsoft wanted to address, however, is that many of those objects and scenes were isolated in the image in such a way that they would almost never be seen in the context of a normal day. Here's an example from the paper. \n",
    "![iconic images vs non-iconic](iconic_vs_non.png)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 4-cluster of images on the left are similar to what was the \"industry standard\" at the time for object detection, while the middle cluster shows iconic scenes: a yard, a living room, a street taken from *SUN database: Large-scale scene recognition from abbey to zoo*. The problem is that many of these images are too idealized, as if they were taken by a professional photographer for a catalog or a real estate listing. What the researchers from Microsoft were more interested in, though, is the non-iconic imagry displayed in the cluster toward the right side. For computer vision to be effective, we need training data that closely matches the types of tasks we want to predict on in the future. Since that data-set didn't exist, a few bright minds at Microsoft decided to build it. \n",
    "\n",
    "Now that you know their motivations, I'll talk about their methods, but first, let's go into a bit more depth on the state of large computer vision datasets at the time this paper was released. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Data Set Name & Link                                            | # of Images | # of Labeled Categories | Year Released | Pros                                                       | Cons                                                                                                 |\n",
    "|-----------------------------------------------------------------|-------------|-----------------|---------------|------------------------------------------------------------|------------------------------------------------------------------------------------------------------|\n",
    "| [ImageNet](http://www.image-net.org/about-overview)             | 14,197,122  | 1000            | 2009          | Huge number of categories & images                         | Subjects are mostly isolated, out of normal context                                                  |\n",
    "| [Sun Database](https://vision.princeton.edu/projects/2010/SUN/) | 131,067     | 397             | 2010          | High 'image complexity' with multiple subjects per photo   | Fewer overall instances for any one category                                                         |\n",
    "| [PASCAL VOC Dataset](http://host.robots.ox.ac.uk/pascal/VOC/)   | 500,000     | 20              | 2012          | Fairly high instances per category                         | Low 'image complexity', and relatively few categories                                                |\n",
    "| [MS COCO](http://cocodataset.org/#home)                         | 2,500,000   | 91              | 2015          | Good 'image complexity', high # of instances per category  | The # of categories could be higher, but overall not a lot of downsides at the time it was released. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Side note: Most of the images in this blog post are copied straight from the academic paper linked above. If you want to know the details of their methodology, I encourage to read the paper itself. It is well written and fairly short, only about 9 pages of text (including the appendix), and lots of insightful pictures and graphs. Here is their informationally dense comparison of the 4 large image datasets. My favorite graph here is the scatterplot in the lower left. Note the log scale on both X & Y axes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![comparison](image_db_comparison.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hope you enjoyed nerding out over these graphs as much as I did. Now that we're all in a good head-space, let's dive into the immense planning and effort that went into creating MS COCO. For brevity, I will do this in an outline.   \n",
    "\n",
    "### How It Was Done\n",
    "1) Realize in what way the current data sets were lacking, and inspire someone to commit the resources to address those shortages  \n",
    "  \n",
    "  * Low image complexity  \n",
    "  * Labels are bounding boxes only, not pixelated shading    \n",
    "  \n",
    "2) Find pictures of multiple objects / scenes in an \"everyday\" context, instead of staged scenes  \n",
    "  \n",
    "  * [Flickr](https://www.flickr.com/about) contains photos uploaded by amateur photographers with searchable metadata and keywords  \n",
    "  * Searches for multiple objects at the same time instead of just a single word / topic  \n",
    "    * *cat wine glass* is better than just *cat* or *wine glass*  \n",
    "\n",
    "3) Annotate Images by hand: [Amazon's Mechanical Turk (AMT)](https://www.mturk.com/) is a platform to crowdsource work, but care must be taken to maintain high standards  \n",
    "\n",
    "A) Label Categories -- Is there a *____________* anywhere in this picture?  \n",
    "   * repeated 8 times per question with different AMT workers, to increase Recall. The chance that all 8 workers \"get it wrong\" is very low.  \n",
    "   \n",
    "B) Instance Spotting -- There is already a *____________* in this picture, place a marker over every discrete instance of it that you see.  \n",
    "   * also repeated 8 times per question with different AMT workers.  \n",
    "   \n",
    "C) Instance Segmentation -- Shade the photo where each individual *____________* takes place (on average, any single photo has 7.7 instances that need to be shaded)  \n",
    "   * Design an interface that will allow workers to shade the specific pixels where an object is in the image.  \n",
    "   * Segmenting 2,500,000 object instances is an extremely time consuming task requiring over 22 worker hours per 1,000 segmentations. (around 80 sec per instance)  \n",
    "     * For this reason, only a single worker will segment each instance, but high training and quality control is required.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we look at the outcomes, I want to remind you of _Werlindo's Saturation Phenomenon_:\n",
    "\n",
    "1) _Exhibit A_ is super novel and cool and ppl love it  \n",
    "2) Everyone copies the effect of _Exhibit A_ to the point that it's overused and perceived as tacky  \n",
    "3) Someone who wasn't around / aware of _Exhibit A_ sees it for the first time, and thinks \"that looks tacky\" not realizing they are looking at something kindof amazing that revolutionized an experience.  \n",
    "\n",
    "I first heard of this with regard to 'Bullet Time' from _The Matrix_ \n",
    "\n",
    "![Bullet time](http://i.imgur.com/4nvICeP.gif)\n",
    "\n",
    "vs the 'low budget' version.\n",
    "\n",
    "![LEGO bullet time](https://media0.giphy.com/media/13ihNcbag4xLt6/giphy.gif)\n",
    "\n",
    "I bring this up, because even though we've seen a lot of cool [image segmentation gifs](https://liuziwei7.github.io/projects/vsreid/demo.gif), to the point of thinking they are normal, none of this would be possible without the rather boring task of building a massive, intricately labeled, natural scene image data set. Thanks, Microsoft COCO. \n",
    "\n",
    "I was going to add even more images to this blog showing the output of this project, but instead, you should probably just [play around](http://cocodataset.org/#explore) with the dataset yourself. There are even [pre-trained models](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) that work \"out of the box\" to leverage all of this hard work, with almost no added input from you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
